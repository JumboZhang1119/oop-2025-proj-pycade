{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.8.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "開始訓練 - 總回合數: 500\n",
      "設備: cuda\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/500 [00:00<?, ?it/s]C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_8664\\1642193708.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states).to(self.device)\n",
      "Training:   2%|▋                               | 10/500 [00:29<24:46,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10/500 | Reward:   9.89 | Avg(100):   9.88 | Epsilon: 0.100 | Steps: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▉                               | 14/500 [00:42<24:32,  3.03s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 你原有的代碼保持不變，這裡添加改進和可視化功能\n",
    "\n",
    "# ---------- 改進的環境類 -----------\n",
    "class RLGameEnv:\n",
    "    def __init__(self, render_mode=False):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((640, 480))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.render_mode = render_mode\n",
    "        # 假設你的Game類存在，這裡保持原有邏輯\n",
    "        # self.game = Game(self.screen, self.clock, ai_archetype=\"rl\")\n",
    "        self.action_space = 5\n",
    "        self.state_dim = (10,)\n",
    "        \n",
    "        # 添加統計信息\n",
    "        self.episode_stats = {\n",
    "            'wins': 0,\n",
    "            'losses': 0,\n",
    "            'draws': 0,\n",
    "            'total_episodes': 0\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        # self.game.setup_initial_state()\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # 原有邏輯保持不變\n",
    "        # 這裡添加更詳細的獎勵設計\n",
    "        state = self.get_state()\n",
    "        reward, done = self.compute_reward_done()\n",
    "        \n",
    "        # 添加形狀獎勵（shaping reward）\n",
    "        reward += self.compute_shaping_reward(action)\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def compute_shaping_reward(self, action):\n",
    "        \"\"\"計算形狀獎勵，幫助AI學習更好的策略\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 獎勵存活時間\n",
    "        reward += 0.01\n",
    "        \n",
    "        # 獎勵靠近敵人（鼓勵攻擊性）\n",
    "        # p2 = self.game.player2_ai\n",
    "        # p1 = self.game.player1\n",
    "        # if p2 and p1:\n",
    "        #     distance = abs(p2.rect.x - p1.rect.x) + abs(p2.rect.y - p1.rect.y)\n",
    "        #     reward += max(0, (200 - distance) / 1000)  # 距離越近獎勵越高\n",
    "        \n",
    "        # 懲罰無意義的移動\n",
    "        if action == 0:  # 停留\n",
    "            reward -= 0.005\n",
    "            \n",
    "        return reward\n",
    "\n",
    "    def get_state(self):\n",
    "        # 原有邏輯，可以考慮添加更多特徵\n",
    "        # 比如：炸彈位置、道具位置、牆壁信息等\n",
    "        return np.zeros(self.state_dim, dtype=np.float32)\n",
    "\n",
    "    def compute_reward_done(self):\n",
    "        # 原有邏輯保持不變\n",
    "        done = False  # self.game.game_state in [\"GAME_OVER\", \"SCORE_SUBMITTED\"]\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 更新統計信息\n",
    "        if done:\n",
    "            self.episode_stats['total_episodes'] += 1\n",
    "            # 根據遊戲結果更新統計\n",
    "            \n",
    "        return reward, done\n",
    "    \n",
    "# ---------- Q Network ----------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ---------- 改進的DQN Agent ----------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        \n",
    "        # 使用優先經驗回放\n",
    "        self.memory = deque(maxlen=50000)  # 增大經驗池\n",
    "        self.batch_size = 64\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_target_steps = 200  # 增加更新頻率\n",
    "        self.step_count = 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_net.to(self.device)\n",
    "        self.target_net.to(self.device)\n",
    "        \n",
    "        # 添加訓練統計\n",
    "        self.training_stats = {\n",
    "            'losses': [],\n",
    "            'q_values': [],\n",
    "            'epsilon_history': []\n",
    "        }\n",
    "        \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_tensor)\n",
    "            \n",
    "        # 記錄Q值用於分析\n",
    "        self.training_stats['q_values'].append(q_values.max().item())\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # 使用Double DQN\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_net(next_states).argmax(1).unsqueeze(1)\n",
    "            max_next_q = self.target_net(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 記錄損失\n",
    "        self.training_stats['losses'].append(loss.item())\n",
    "        \n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        self.training_stats['epsilon_history'].append(self.epsilon)\n",
    "\n",
    "        # 更新 target net\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.update_target_steps == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"保存模型和訓練統計\"\"\"\n",
    "        torch.save({\n",
    "            'q_net_state_dict': self.q_net.state_dict(),\n",
    "            'target_net_state_dict': self.target_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'training_stats': self.training_stats,\n",
    "            'epsilon': self.epsilon,\n",
    "            'step_count': self.step_count\n",
    "        }, filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"加載模型\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_net.load_state_dict(checkpoint['q_net_state_dict'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.training_stats = checkpoint['training_stats']\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.step_count = checkpoint['step_count']\n",
    "\n",
    "# ---------- 可視化和分析工具 ----------\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        self.fig = None\n",
    "        \n",
    "    def plot_training_results(self, rewards_log, agent, env, save_path=\"training_results.png\"):\n",
    "        \"\"\"繪製訓練結果\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('炸彈人RL訓練結果分析', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. 獎勵曲線\n",
    "        axes[0, 0].plot(rewards_log, alpha=0.7, linewidth=1)\n",
    "        if len(rewards_log) > 20:\n",
    "            # 移動平均\n",
    "            window = min(50, len(rewards_log) // 10)\n",
    "            moving_avg = np.convolve(rewards_log, np.ones(window)/window, mode='valid')\n",
    "            axes[0, 0].plot(range(window-1, len(rewards_log)), moving_avg, \n",
    "                           color='red', linewidth=2, label=f'移動平均({window})')\n",
    "            axes[0, 0].legend()\n",
    "        axes[0, 0].set_title('每回合獎勵')\n",
    "        axes[0, 0].set_xlabel('回合數')\n",
    "        axes[0, 0].set_ylabel('獎勵')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 損失曲線\n",
    "        if agent.training_stats['losses']:\n",
    "            losses = agent.training_stats['losses']\n",
    "            axes[0, 1].plot(losses, alpha=0.7)\n",
    "            if len(losses) > 100:\n",
    "                window = len(losses) // 50\n",
    "                moving_avg = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "                axes[0, 1].plot(range(window-1, len(losses)), moving_avg, \n",
    "                               color='red', linewidth=2)\n",
    "            axes[0, 1].set_title('訓練損失')\n",
    "            axes[0, 1].set_xlabel('訓練步數')\n",
    "            axes[0, 1].set_ylabel('MSE Loss')\n",
    "            axes[0, 1].set_yscale('log')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Epsilon衰減\n",
    "        if agent.training_stats['epsilon_history']:\n",
    "            axes[0, 2].plot(agent.training_stats['epsilon_history'])\n",
    "            axes[0, 2].set_title('探索率(Epsilon)衰減')\n",
    "            axes[0, 2].set_xlabel('訓練步數')\n",
    "            axes[0, 2].set_ylabel('Epsilon')\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Q值統計\n",
    "        if agent.training_stats['q_values']:\n",
    "            q_values = agent.training_stats['q_values']\n",
    "            axes[1, 0].plot(q_values, alpha=0.7)\n",
    "            if len(q_values) > 100:\n",
    "                window = len(q_values) // 50\n",
    "                moving_avg = np.convolve(q_values, np.ones(window)/window, mode='valid')\n",
    "                axes[1, 0].plot(range(window-1, len(q_values)), moving_avg, \n",
    "                               color='red', linewidth=2)\n",
    "            axes[1, 0].set_title('最大Q值變化')\n",
    "            axes[1, 0].set_xlabel('動作選擇次數')\n",
    "            axes[1, 0].set_ylabel('最大Q值')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. 獎勵分布\n",
    "        axes[1, 1].hist(rewards_log, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].axvline(np.mean(rewards_log), color='red', linestyle='--', \n",
    "                          label=f'平均: {np.mean(rewards_log):.2f}')\n",
    "        axes[1, 1].axvline(np.median(rewards_log), color='green', linestyle='--', \n",
    "                          label=f'中位數: {np.median(rewards_log):.2f}')\n",
    "        axes[1, 1].set_title('獎勵分布')\n",
    "        axes[1, 1].set_xlabel('獎勵值')\n",
    "        axes[1, 1].set_ylabel('頻率')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. 勝率統計（如果有環境統計）\n",
    "        if hasattr(env, 'episode_stats') and env.episode_stats['total_episodes'] > 0:\n",
    "            stats = env.episode_stats\n",
    "            labels = ['勝利', '失敗', '平局']\n",
    "            sizes = [stats['wins'], stats['losses'], stats['draws']]\n",
    "            colors = ['gold', 'lightcoral', 'lightblue']\n",
    "            \n",
    "            # 只顯示非零的項目\n",
    "            non_zero_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "            if non_zero_data:\n",
    "                labels, sizes, colors = zip(*non_zero_data)\n",
    "                axes[1, 2].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "                axes[1, 2].set_title('遊戲結果分布')\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, '暫無統計數據', ha='center', va='center', fontsize=14)\n",
    "            axes[1, 2].set_title('遊戲結果分布')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_action_distribution(self, action_history, save_path=\"action_distribution.png\"):\n",
    "        \"\"\"分析動作選擇分布\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        action_names = ['停留', '向上', '向下', '向左', '向右']\n",
    "        action_counts = np.bincount(action_history, minlength=5)\n",
    "        \n",
    "        bars = plt.bar(action_names, action_counts, color=['gray', 'blue', 'green', 'orange', 'red'])\n",
    "        plt.title('動作選擇分布', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('動作類型')\n",
    "        plt.ylabel('選擇次數')\n",
    "        \n",
    "        # 添加數值標籤\n",
    "        for bar, count in zip(bars, action_counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(action_counts)*0.01,\n",
    "                    str(count), ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# ---------- 改進的訓練函數 ----------\n",
    "def train(env, num_episodes=1000, save_interval=100, visualize=True):\n",
    "    \"\"\"改進的訓練函數\"\"\"\n",
    "    agent = DQNAgent(state_dim=env.state_dim[0], action_dim=env.action_space)\n",
    "    visualizer = TrainingVisualizer() if visualize else None\n",
    "    \n",
    "    rewards_log = []\n",
    "    action_history = []\n",
    "    best_avg_reward = float('-inf')\n",
    "    \n",
    "    print(f\"開始訓練 - 總回合數: {num_episodes}\")\n",
    "    print(f\"設備: {agent.device}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training\", ncols=80, dynamic_ncols=False):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_actions = []\n",
    "        done = False\n",
    "        steps = 0\n",
    "        max_steps = 1000  # 防止無限循環\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.store(state, action, reward, next_state, done)\n",
    "            agent.train_step()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_actions.append(action)\n",
    "            steps += 1\n",
    "            \n",
    "            if env.render_mode:\n",
    "                env.render()\n",
    "\n",
    "        rewards_log.append(total_reward)\n",
    "        action_history.extend(episode_actions)\n",
    "        \n",
    "        # 計算最近100回合的平均獎勵\n",
    "        recent_avg = np.mean(rewards_log[-100:]) if len(rewards_log) >= 100 else np.mean(rewards_log)\n",
    "        \n",
    "        # 每10回合打印一次進度\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode+1:4d}/{num_episodes} | \"\n",
    "                  f\"Reward: {total_reward:6.2f} | \"\n",
    "                  f\"Avg(100): {recent_avg:6.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Steps: {steps:3d}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if recent_avg > best_avg_reward:\n",
    "            best_avg_reward = recent_avg\n",
    "            agent.save_model(\"best_model.pt\")\n",
    "            \n",
    "        # 定期保存和可視化\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            # 保存檢查點\n",
    "            agent.save_model(f\"checkpoint_episode_{episode+1}.pt\")\n",
    "            \n",
    "            # 生成可視化\n",
    "            if visualizer:\n",
    "                print(f\"\\n生成訓練進度可視化...\")\n",
    "                visualizer.plot_training_results(rewards_log, agent, env, \n",
    "                                                f\"training_progress_ep{episode+1}.png\")\n",
    "                \n",
    "            print(f\"已保存檢查點和可視化結果\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    # 訓練完成後的最終分析\n",
    "    print(f\"\\n訓練完成！\")\n",
    "    print(f\"最佳平均獎勵: {best_avg_reward:.2f}\")\n",
    "    print(f\"最終平均獎勵: {np.mean(rewards_log[-100:]):.2f}\")\n",
    "    \n",
    "    if visualizer:\n",
    "        print(\"生成最終分析報告...\")\n",
    "        visualizer.plot_training_results(rewards_log, agent, env, \"final_training_results.png\")\n",
    "        visualizer.plot_action_distribution(action_history, \"action_distribution.png\")\n",
    "    \n",
    "    # 保存訓練日誌\n",
    "    training_log = {\n",
    "        'rewards': rewards_log,\n",
    "        'training_stats': agent.training_stats,\n",
    "        'env_stats': env.episode_stats if hasattr(env, 'episode_stats') else {},\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': 1e-3,\n",
    "            'gamma': 0.99,\n",
    "            'epsilon_decay': 0.995,\n",
    "            'batch_size': agent.batch_size,\n",
    "            'memory_size': len(agent.memory)\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open('training_log.json', 'w') as f:\n",
    "        json.dump(training_log, f, indent=2)\n",
    "    \n",
    "    return rewards_log, agent\n",
    "\n",
    "# ---------- 模型評估函數 ----------\n",
    "def evaluate_model(env, model_path, num_episodes=100):\n",
    "    \"\"\"評估訓練好的模型\"\"\"\n",
    "    agent = DQNAgent(state_dim=env.state_dim[0], action_dim=env.action_space)\n",
    "    agent.load_model(model_path)\n",
    "    agent.epsilon = 0  # 評估時不使用隨機動作\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if env.render_mode:\n",
    "                env.render()\n",
    "                \n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "    print(f\"評估結果 (共{num_episodes}回合):\")\n",
    "    print(f\"平均獎勵: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "    print(f\"最佳獎勵: {np.max(rewards):.2f}\")\n",
    "    print(f\"最差獎勵: {np.min(rewards):.2f}\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# ---------- 主程序 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # 創建環境\n",
    "    env = RLGameEnv(render_mode=False)  # 訓練時建議關閉渲染加速\n",
    "    \n",
    "    # 開始訓練\n",
    "    rewards_log, trained_agent = train(env, num_episodes=500, visualize=True)\n",
    "    \n",
    "    # 評估最佳模型\n",
    "    print(\"\\n評估最佳模型:\")\n",
    "    eval_env = RLGameEnv(render_mode=True)  # 評估時可以開啟渲染觀看\n",
    "    evaluate_model(eval_env, \"best_model.pt\", num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.8.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_bomberman_ai() got an unexpected keyword argument 'visualize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 566\u001b[0m\n\u001b[0;32m    563\u001b[0m env \u001b[38;5;241m=\u001b[39m RLGameEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# 訓練時建議關閉渲染加速\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# 開始訓練\u001b[39;00m\n\u001b[1;32m--> 566\u001b[0m rewards_log, trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bomberman_ai\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# 評估最佳模型\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m評估最佳模型:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_bomberman_ai() got an unexpected keyword argument 'visualize'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pygame\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import random\n",
    "# from collections import deque\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # ---------- 改進的環境類 - 包含炸彈機制 -----------\n",
    "# class RLGameEnv:\n",
    "#     def __init__(self, render_mode=False):\n",
    "#         pygame.init()\n",
    "#         self.screen = pygame.display.set_mode((640, 480))\n",
    "#         self.clock = pygame.time.Clock()\n",
    "#         self.render_mode = render_mode\n",
    "#         # self.game = Game(self.screen, self.clock, ai_archetype=\"rl\")\n",
    "        \n",
    "#         # 擴展動作空間 - 增加放炸彈\n",
    "#         self.action_space = 6  # 0:停留, 1:上, 2:下, 3:左, 4:右, 5:放炸彈\n",
    "        \n",
    "#         # 擴展狀態空間 - 包含更多炸彈人相關信息\n",
    "#         self.state_dim = (20,)  # 增加到20維狀態\n",
    "        \n",
    "#         # 遊戲統計\n",
    "#         self.episode_stats = {\n",
    "#             'wins': 0,\n",
    "#             'losses': 0,\n",
    "#             'draws': 0,\n",
    "#             'bombs_placed': 0,\n",
    "#             'enemies_killed_by_bomb': 0,\n",
    "#             'self_killed_by_bomb': 0,\n",
    "#             'total_episodes': 0\n",
    "#         }\n",
    "        \n",
    "#         # 上一步的信息（用於計算獎勵）\n",
    "#         self.prev_enemy_lives = None\n",
    "#         self.prev_ai_lives = None\n",
    "#         self.prev_distance_to_enemy = None\n",
    "\n",
    "#     def reset(self):\n",
    "#         # self.game.setup_initial_state()\n",
    "#         self.prev_enemy_lives = None\n",
    "#         self.prev_ai_lives = None\n",
    "#         self.prev_distance_to_enemy = None\n",
    "#         return self.get_state()\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # 記錄動作前的狀態\n",
    "#         prev_state = self.get_state()\n",
    "        \n",
    "#         # 應用動作到AI玩家\n",
    "#         if self.game.player2_ai and self.game.player2_ai.is_alive:\n",
    "#             self.apply_action(action)\n",
    "        \n",
    "#         # 更新遊戲狀態\n",
    "#         self.game.update()\n",
    "        \n",
    "#         # 獲取新狀態\n",
    "#         new_state = self.get_state()\n",
    "        \n",
    "#         # 計算獎勵和結束條件\n",
    "#         reward, done = self.compute_reward_done(action, prev_state)\n",
    "        \n",
    "#         return new_state, reward, done, {}\n",
    "\n",
    "#     def apply_action(self, action):\n",
    "#         \"\"\"應用動作到AI玩家\"\"\"\n",
    "#         ai = self.game.player2_ai\n",
    "#         if not ai:\n",
    "#             return\n",
    "            \n",
    "#         if action == 0:\n",
    "#             pass  # 停留\n",
    "#         elif action == 1:\n",
    "#             ai.move(0, -1)  # 向上\n",
    "#         elif action == 2:\n",
    "#             ai.move(0, 1)   # 向下\n",
    "#         elif action == 3:\n",
    "#             ai.move(-1, 0)  # 向左\n",
    "#         elif action == 4:\n",
    "#             ai.move(1, 0)   # 向右\n",
    "#         elif action == 5:\n",
    "#             # 放置炸彈 - 模擬按下F鍵\n",
    "#             self.place_bomb(ai)\n",
    "#             self.episode_stats['bombs_placed'] += 1\n",
    "\n",
    "#     def place_bomb(self, player):\n",
    "#         \"\"\"\n",
    "#         放置炸彈的邏輯 - 你需要根據你的遊戲實現來調整\n",
    "#         這裡假設你的Game類有相應的方法\n",
    "#         \"\"\"\n",
    "#         # 假設你的遊戲有這樣的方法：\n",
    "#         # self.game.place_bomb(player)\n",
    "        \n",
    "#         # 或者直接觸發按鍵事件：\n",
    "#         # pygame.event.post(pygame.event.Event(pygame.KEYDOWN, key=pygame.K_f))\n",
    "        \n",
    "#         # 這裡你需要根據你的實際遊戲代碼來實現\n",
    "#         pass\n",
    "\n",
    "#     def get_state(self):\n",
    "#         \"\"\"\n",
    "#         獲取更詳細的遊戲狀態 - 專為炸彈人遊戲設計\n",
    "#         \"\"\"\n",
    "#         # 如果遊戲未初始化，返回零狀態\n",
    "#         if not hasattr(self, 'game') or not self.game:\n",
    "#             return np.zeros(self.state_dim, dtype=np.float32)\n",
    "            \n",
    "#         p2 = self.game.player2_ai  # AI玩家\n",
    "#         p1 = self.game.player1     # 人類玩家\n",
    "        \n",
    "#         if p2 is None or p1 is None:\n",
    "#             return np.zeros(self.state_dim, dtype=np.float32)\n",
    "        \n",
    "#         # 基本位置和狀態信息\n",
    "#         state_list = [\n",
    "#             # AI玩家信息\n",
    "#             p2.rect.x / 640.0,           # AI位置X\n",
    "#             p2.rect.y / 480.0,           # AI位置Y\n",
    "#             p2.lives / 3.0,              # AI生命值（假設最大3條命）\n",
    "#             p2.score / 1000.0,           # AI分數\n",
    "#             int(p2.is_alive),            # AI是否存活\n",
    "            \n",
    "#             # 敵人信息\n",
    "#             p1.rect.x / 640.0,           # 敵人位置X\n",
    "#             p1.rect.y / 480.0,           # 敵人位置Y\n",
    "#             p1.lives / 3.0,              # 敵人生命值\n",
    "#             p1.score / 1000.0,           # 敵人分數\n",
    "#             int(p1.is_alive),            # 敵人是否存活\n",
    "            \n",
    "#             # 遊戲全局信息\n",
    "#             self.game.time_elapsed_seconds / 120.0,  # 遊戲時間（假設2分鐘）\n",
    "#         ]\n",
    "        \n",
    "#         # 距離信息\n",
    "#         distance_to_enemy = abs(p2.rect.x - p1.rect.x) + abs(p2.rect.y - p1.rect.y)\n",
    "#         state_list.append(distance_to_enemy / 1000.0)  # 正規化距離\n",
    "        \n",
    "#         # 炸彈相關狀態（如果你的遊戲有炸彈列表）\n",
    "#         # 這裡需要根據你的遊戲實現來調整\n",
    "#         bomb_states = self.get_bomb_states()\n",
    "#         state_list.extend(bomb_states)\n",
    "        \n",
    "#         # 地圖信息（周圍的牆壁、道具等）\n",
    "#         surrounding_info = self.get_surrounding_info(p2)\n",
    "#         state_list.extend(surrounding_info)\n",
    "        \n",
    "#         # 確保狀態維度正確\n",
    "#         state_array = np.array(state_list[:self.state_dim[0]], dtype=np.float32)\n",
    "        \n",
    "#         # 如果不足維度，用0填充\n",
    "#         if len(state_array) < self.state_dim[0]:\n",
    "#             padding = np.zeros(self.state_dim[0] - len(state_array), dtype=np.float32)\n",
    "#             state_array = np.concatenate([state_array, padding])\n",
    "            \n",
    "#         return state_array\n",
    "\n",
    "#     def get_bomb_states(self):\n",
    "#         \"\"\"\n",
    "#         獲取炸彈相關狀態信息\n",
    "#         你需要根據你的遊戲實現來調整\n",
    "#         \"\"\"\n",
    "#         bomb_info = []\n",
    "        \n",
    "#         # 假設你的遊戲有炸彈列表\n",
    "#         # if hasattr(self.game, 'bombs'):\n",
    "#         #     # 最近炸彈的距離和剩餘時間\n",
    "#         #     ai_pos = (self.game.player2_ai.rect.x, self.game.player2_ai.rect.y)\n",
    "#         #     nearest_bomb_dist = float('inf')\n",
    "#         #     nearest_bomb_time = 0\n",
    "#         #     \n",
    "#         #     for bomb in self.game.bombs:\n",
    "#         #         dist = abs(bomb.x - ai_pos[0]) + abs(bomb.y - ai_pos[1])\n",
    "#         #         if dist < nearest_bomb_dist:\n",
    "#         #             nearest_bomb_dist = dist\n",
    "#         #             nearest_bomb_time = bomb.timer\n",
    "#         #     \n",
    "#         #     bomb_info = [\n",
    "#         #         min(nearest_bomb_dist / 200.0, 1.0),  # 最近炸彈距離\n",
    "#         #         nearest_bomb_time / 100.0,            # 最近炸彈剩餘時間\n",
    "#         #         len(self.game.bombs) / 10.0           # 場上炸彈數量\n",
    "#         #     ]\n",
    "#         # else:\n",
    "#         bomb_info = [0.0, 0.0, 0.0]  # 默認值\n",
    "        \n",
    "#         return bomb_info\n",
    "\n",
    "#     def get_surrounding_info(self, player):\n",
    "#         \"\"\"\n",
    "#         獲取玩家周圍的環境信息\n",
    "#         \"\"\"\n",
    "#         surrounding = []\n",
    "        \n",
    "#         # 檢查周圍8個方向是否有障礙物\n",
    "#         directions = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
    "        \n",
    "#         for dx, dy in directions:\n",
    "#             # 這裡需要根據你的地圖實現來檢查\n",
    "#             # is_blocked = self.game.is_position_blocked(player.rect.x + dx*32, player.rect.y + dy*32)\n",
    "#             # surrounding.append(float(is_blocked))\n",
    "#             surrounding.append(0.0)  # 暫時用0填充\n",
    "            \n",
    "#         return surrounding[:5]  # 只取前5個方向\n",
    "\n",
    "#     def compute_reward_done(self, action, prev_state):\n",
    "#         \"\"\"\n",
    "#         計算獎勵和判斷是否結束 - 專為炸彈人遊戲設計的獎勵函數\n",
    "#         \"\"\"\n",
    "#         done = self.game.game_state in [\"GAME_OVER\", \"SCORE_SUBMITTED\"]\n",
    "#         reward = 0.0\n",
    "        \n",
    "#         p2 = self.game.player2_ai\n",
    "#         p1 = self.game.player1\n",
    "        \n",
    "#         if p2 is None or p1 is None:\n",
    "#             return 0.0, True\n",
    "        \n",
    "#         # === 1. 遊戲結果獎勵（最重要） ===\n",
    "#         if done:\n",
    "#             self.episode_stats['total_episodes'] += 1\n",
    "#             if self.game.time_up_winner == \"AI\":\n",
    "#                 reward += 100.0  # 大獎勵：獲勝\n",
    "#                 self.episode_stats['wins'] += 1\n",
    "#             elif self.game.time_up_winner == \"P1\":\n",
    "#                 reward -= 100.0  # 大懲罰：失敗\n",
    "#                 self.episode_stats['losses'] += 1\n",
    "#             elif self.game.time_up_winner == \"DRAW\":\n",
    "#                 reward += 10.0   # 小獎勵：平局\n",
    "#                 self.episode_stats['draws'] += 1\n",
    "        \n",
    "#         # === 2. 生命值變化獎勵 ===\n",
    "#         current_ai_lives = p2.lives\n",
    "#         current_enemy_lives = p1.lives\n",
    "        \n",
    "#         if self.prev_ai_lives is not None:\n",
    "#             # AI失去生命 - 大懲罰\n",
    "#             if current_ai_lives < self.prev_ai_lives:\n",
    "#                 reward -= 50.0\n",
    "#                 # 如果是被自己炸彈炸死，額外懲罰\n",
    "#                 if action == 5:  # 剛才放了炸彈\n",
    "#                     reward -= 20.0\n",
    "#                     self.episode_stats['self_killed_by_bomb'] += 1\n",
    "            \n",
    "#             # 敵人失去生命 - 大獎勵\n",
    "#             if current_enemy_lives < self.prev_enemy_lives:\n",
    "#                 reward += 50.0\n",
    "#                 self.episode_stats['enemies_killed_by_bomb'] += 1\n",
    "        \n",
    "#         # === 3. 距離相關獎勵 ===\n",
    "#         current_distance = abs(p2.rect.x - p1.rect.x) + abs(p2.rect.y - p1.rect.y)\n",
    "        \n",
    "#         if self.prev_distance_to_enemy is not None:\n",
    "#             # 靠近敵人獎勵（鼓勵攻擊性）\n",
    "#             if current_distance < self.prev_distance_to_enemy:\n",
    "#                 reward += 0.5\n",
    "#             # 遠離敵人小懲罰\n",
    "#             elif current_distance > self.prev_distance_to_enemy:\n",
    "#                 reward -= 0.2\n",
    "        \n",
    "#         # === 4. 動作相關獎勵 ===\n",
    "#         # 鼓勵積極行動\n",
    "#         if action == 0:  # 停留\n",
    "#             reward -= 0.1\n",
    "#         elif action == 5:  # 放炸彈\n",
    "#             reward += 1.0  # 鼓勵使用炸彈\n",
    "            \n",
    "#             # 如果在敵人附近放炸彈，額外獎勵\n",
    "#             if current_distance < 100:  # 假設100像素內算\"附近\"\n",
    "#                 reward += 2.0\n",
    "            \n",
    "#             # 如果太靠近自己放炸彈，小懲罰（危險）\n",
    "#             if current_distance < 50:\n",
    "#                 reward -= 1.0\n",
    "        \n",
    "#         # === 5. 存活時間獎勵 ===\n",
    "#         if p2.is_alive:\n",
    "#             reward += 0.01  # 每個時間步的小獎勵\n",
    "        \n",
    "#         # === 6. 分數變化獎勵 ===\n",
    "#         # 如果你的遊戲有分數系統\n",
    "#         # score_diff = p2.score - p1.score\n",
    "#         # reward += score_diff * 0.01\n",
    "        \n",
    "#         # === 7. 安全性考慮 ===\n",
    "#         # 如果AI在炸彈爆炸範圍內，給予懲罰\n",
    "#         danger_penalty = self.calculate_danger_penalty(p2)\n",
    "#         reward += danger_penalty\n",
    "        \n",
    "#         # 更新上一步信息\n",
    "#         self.prev_ai_lives = current_ai_lives\n",
    "#         self.prev_enemy_lives = current_enemy_lives\n",
    "#         self.prev_distance_to_enemy = current_distance\n",
    "        \n",
    "#         return reward, done\n",
    "\n",
    "#     def calculate_danger_penalty(self, player):\n",
    "#         \"\"\"\n",
    "#         計算危險懲罰 - 如果AI在炸彈爆炸範圍內\n",
    "#         \"\"\"\n",
    "#         penalty = 0.0\n",
    "        \n",
    "#         # 這裡需要根據你的炸彈爆炸機制來實現\n",
    "#         # if hasattr(self.game, 'bombs'):\n",
    "#         #     for bomb in self.game.bombs:\n",
    "#         #         # 檢查是否在爆炸範圍內\n",
    "#         #         if self.is_in_blast_range(player, bomb):\n",
    "#         #             # 根據炸彈剩餘時間給予不同程度的懲罰\n",
    "#         #             time_factor = max(0, 1 - bomb.timer / 100.0)\n",
    "#         #             penalty -= 5.0 * time_factor\n",
    "        \n",
    "#         return penalty\n",
    "\n",
    "#     def is_in_blast_range(self, player, bomb):\n",
    "#         \"\"\"\n",
    "#         檢查玩家是否在炸彈爆炸範圍內\n",
    "#         \"\"\"\n",
    "#         # 這裡需要根據你的遊戲爆炸機制來實現\n",
    "#         # 假設爆炸範圍是十字形，每個方向3格\n",
    "#         blast_range = 3 * 32  # 假設每格32像素\n",
    "        \n",
    "#         # 檢查是否在同一行或同一列，且在爆炸範圍內\n",
    "#         same_row = abs(player.rect.y - bomb.y) < 16  # 允許小誤差\n",
    "#         same_col = abs(player.rect.x - bomb.x) < 16\n",
    "        \n",
    "#         if same_row:\n",
    "#             return abs(player.rect.x - bomb.x) <= blast_range\n",
    "#         elif same_col:\n",
    "#             return abs(player.rect.y - bomb.y) <= blast_range\n",
    "        \n",
    "#         return False\n",
    "\n",
    "# # ---------- 改進的Q網絡 - 適應炸彈人遊戲 ----------\n",
    "# class BombermanQNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(BombermanQNetwork, self).__init__()\n",
    "        \n",
    "#         # 更深的網絡結構，適合複雜的炸彈人策略\n",
    "#         self.feature_layers = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         # 分別處理移動和攻擊動作\n",
    "#         self.movement_head = nn.Linear(128, 5)  # 0-4: 停留和移動\n",
    "#         self.bomb_head = nn.Linear(128, 1)      # 5: 放炸彈\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         features = self.feature_layers(x)\n",
    "#         movement_q = self.movement_head(features)\n",
    "#         bomb_q = self.bomb_head(features)\n",
    "        \n",
    "#         # 合併輸出\n",
    "#         q_values = torch.cat([movement_q, bomb_q], dim=1)\n",
    "#         return q_values\n",
    "\n",
    "# # ---------- 專用的炸彈人DQN智能體 ----------\n",
    "# class BombermanDQNAgent:\n",
    "#     def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, \n",
    "#                  epsilon_min=0.1, epsilon_decay=0.995):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "        \n",
    "#         # 使用專門的炸彈人網絡\n",
    "#         self.q_net = BombermanQNetwork(state_dim, action_dim)\n",
    "#         self.target_net = BombermanQNetwork(state_dim, action_dim)\n",
    "#         self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        \n",
    "#         # 使用較小的學習率，因為炸彈人策略比較複雜\n",
    "#         self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "#         # 更大的經驗回放池\n",
    "#         self.memory = deque(maxlen=100000)\n",
    "#         self.batch_size = 128  # 增大批次大小\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.update_target_steps = 500  # 更頻繁的目標網絡更新\n",
    "#         self.step_count = 0\n",
    "        \n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.q_net.to(self.device)\n",
    "#         self.target_net.to(self.device)\n",
    "        \n",
    "#         # 動作統計\n",
    "#         self.action_stats = {i: 0 for i in range(action_dim)}\n",
    "        \n",
    "#         print(f\"炸彈人DQN智能體初始化完成\")\n",
    "#         print(f\"狀態維度: {state_dim}, 動作維度: {action_dim}\")\n",
    "#         print(f\"設備: {self.device}\")\n",
    "\n",
    "#     def select_action(self, state, training=True):\n",
    "#         \"\"\"選擇動作，在訓練時使用epsilon-greedy策略\"\"\"\n",
    "#         if training and np.random.rand() < self.epsilon:\n",
    "#             # 隨機動作，但偏向於移動而不是放炸彈\n",
    "#             if np.random.rand() < 0.8:\n",
    "#                 action = np.random.randint(0, 5)  # 移動動作\n",
    "#             else:\n",
    "#                 action = 5  # 放炸彈\n",
    "#         else:\n",
    "#             state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "#             with torch.no_grad():\n",
    "#                 q_values = self.q_net(state_tensor)\n",
    "#             action = q_values.argmax().item()\n",
    "        \n",
    "#         # 記錄動作統計\n",
    "#         self.action_stats[action] += 1\n",
    "#         return action\n",
    "\n",
    "#     def store(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#     def train_step(self):\n",
    "#         if len(self.memory) < self.batch_size:\n",
    "#             return None\n",
    "            \n",
    "#         batch = random.sample(self.memory, self.batch_size)\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "#         states = torch.FloatTensor(states).to(self.device)\n",
    "#         actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "#         rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "#         next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "#         dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "#         # Double DQN\n",
    "#         q_values = self.q_net(states).gather(1, actions)\n",
    "#         with torch.no_grad():\n",
    "#             next_actions = self.q_net(next_states).argmax(1).unsqueeze(1)\n",
    "#             max_next_q = self.target_net(next_states).gather(1, next_actions)\n",
    "#             target_q = rewards + (1 - dones) * self.gamma * max_next_q\n",
    "\n",
    "#         # Huber Loss（對異常值更魯棒）\n",
    "#         loss = nn.SmoothL1Loss()(q_values, target_q)\n",
    "        \n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # 梯度裁剪\n",
    "#         torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         # Epsilon衰減\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "\n",
    "#         # 更新目標網絡\n",
    "#         self.step_count += 1\n",
    "#         if self.step_count % self.update_target_steps == 0:\n",
    "#             self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "#         return loss.item()\n",
    "\n",
    "#     def get_action_distribution(self):\n",
    "#         \"\"\"獲取動作分布統計\"\"\"\n",
    "#         total = sum(self.action_stats.values())\n",
    "#         if total == 0:\n",
    "#             return self.action_stats\n",
    "        \n",
    "#         return {action: count/total for action, count in self.action_stats.items()}\n",
    "\n",
    "# # ---------- 主訓練函數 ----------\n",
    "# def train_bomberman_ai(env, num_episodes=1000, save_interval=100, visualize=False):\n",
    "#     visualizer = TrainingVisualizer() if visualize else None\n",
    "#     \"\"\"訓練炸彈人AI\"\"\"\n",
    "#     agent = BombermanDQNAgent(\n",
    "#         state_dim=env.state_dim[0], \n",
    "#         action_dim=env.action_space,\n",
    "#         lr=5e-4,  # 較小的學習率\n",
    "#         gamma=0.95,  # 稍微降低折扣因子\n",
    "#         epsilon_decay=0.9995  # 較慢的探索衰減\n",
    "#     )\n",
    "    \n",
    "#     rewards_log = []\n",
    "#     loss_log = []\n",
    "#     episode_lengths = []\n",
    "    \n",
    "#     print(f\"開始訓練炸彈人AI - 總回合數: {num_episodes}\")\n",
    "#     print(f\"動作空間: {['停留', '上', '下', '左', '右', '放炸彈']}\")\n",
    "#     print(\"-\" * 60)\n",
    "    \n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()\n",
    "#         total_reward = 0\n",
    "#         episode_loss = []\n",
    "#         steps = 0\n",
    "#         max_steps = 2000  # 炸彈人遊戲可能較長\n",
    "#         done = False\n",
    "\n",
    "#         while not done and steps < max_steps:\n",
    "#             action = agent.select_action(state, training=True)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "#             agent.store(state, action, reward, next_state, done)\n",
    "#             loss = agent.train_step()\n",
    "#             if loss is not None:\n",
    "#                 episode_loss.append(loss)\n",
    "            \n",
    "#             state = next_state\n",
    "#             total_reward += reward\n",
    "#             steps += 1\n",
    "            \n",
    "#             if env.render_mode and episode % 50 == 0:  # 每50回合渲染一次\n",
    "#                 env.render()\n",
    "\n",
    "#         rewards_log.append(total_reward)\n",
    "#         episode_lengths.append(steps)\n",
    "#         if episode_loss:\n",
    "#             loss_log.append(np.mean(episode_loss))\n",
    "        \n",
    "#         # 計算統計信息\n",
    "#         recent_avg_reward = np.mean(rewards_log[-100:]) if len(rewards_log) >= 100 else np.mean(rewards_log)\n",
    "#         recent_avg_length = np.mean(episode_lengths[-100:]) if len(episode_lengths) >= 100 else np.mean(episode_lengths)\n",
    "        \n",
    "#         # 每10回合報告一次\n",
    "#         if (episode + 1) % 10 == 0:\n",
    "#             action_dist = agent.get_action_distribution()\n",
    "#             bomb_rate = action_dist.get(5, 0) * 100  # 放炸彈的比例\n",
    "            \n",
    "#             print(f\"Episode {episode+1:4d}/{num_episodes} | \"\n",
    "#                   f\"Reward: {total_reward:7.2f} | \"\n",
    "#                   f\"Avg(100): {recent_avg_reward:6.2f} | \"\n",
    "#                   f\"Steps: {steps:3d} | \"\n",
    "#                   f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "#                   f\"Bomb%: {bomb_rate:4.1f}\")\n",
    "        \n",
    "#         # 每100回合詳細報告\n",
    "#         if (episode + 1) % 100 == 0:\n",
    "#             print(f\"\\n=== 第 {episode+1} 回合統計 ===\")\n",
    "#             print(f\"環境統計: {env.episode_stats}\")\n",
    "#             print(f\"動作分布: {['停留', '上', '下', '左', '右', '放炸彈']}\")\n",
    "#             action_dist = agent.get_action_distribution()\n",
    "#             for i, action_name in enumerate(['停留', '上', '下', '左', '右', '放炸彈']):\n",
    "#                 print(f\"  {action_name}: {action_dist.get(i, 0)*100:.1f}%\")\n",
    "#             print(\"-\" * 60)\n",
    "        \n",
    "#         # 保存檢查點\n",
    "#         if (episode + 1) % save_interval == 0:\n",
    "#             torch.save({\n",
    "#                 'q_net_state_dict': agent.q_net.state_dict(),\n",
    "#                 'target_net_state_dict': agent.target_net.state_dict(),\n",
    "#                 'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "#                 'episode': episode,\n",
    "#                 'rewards_log': rewards_log,\n",
    "#                 'env_stats': env.episode_stats,\n",
    "#                 'action_stats': agent.action_stats\n",
    "#             }, f\"bomberman_checkpoint_ep{episode+1}.pt\")\n",
    "\n",
    "#     print(f\"\\n訓練完成！\")\n",
    "#     print(f\"最終統計: {env.episode_stats}\")\n",
    "    \n",
    "#     return rewards_log, agent\n",
    "\n",
    "# # 使用示例\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = RLGameEnv(render_mode=False)  # 訓練時建議關閉渲染加速\n",
    "    \n",
    "#     # 開始訓練\n",
    "#     rewards_log, trained_agent = train_bomberman_ai(env, num_episodes=500, visualize=True)\n",
    "    \n",
    "#     # 評估最佳模型\n",
    "#     print(\"\\n評估最佳模型:\")\n",
    "#     eval_env = RLGameEnv(render_mode=True)  # 評估時可以開啟渲染觀看\n",
    "#     evaluate_model(eval_env, \"best_model.pt\", num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
